Main Method = The entry point of a Java program

public static void main(String[] args) {
        *CODE*
}

public = Needs to be public so that the JRE (Java Runtime Environment) can access and execute it.
static = Needs to be static so that the JVM can load the class into memory and call the main method without creating an instance of the class first.
void = Return type is void because the main function shouldn't return anything, when it finishes executing, the Java program simply terminates.
String[] args = accepts arguments from the command line that are stored in a String array to affect the operation of the program.

----------------------CONCEPTS-------------------------------------------------------------------------------------------------------

Algorithm = A collection of steps to solve a problem
Ex: Linear Search = Examining the elements of an array one by one to find a value.

Data Structures = Named location used to store and organize data
Ex: Array = A collection of elements stores in contiguous memory locations.

Big O notation:
"How code slows as data grows"
1 - Describes the performance of an algorithm as the amount of data increases
2 - Machine independent, it focuses on the number of steps to complete an algorithm, independent of hardware.
3 - Ignores smaller operations Ex: O(n + 1) -> 0(n), the +1 wouldn't make much of a difference.

Examples of notations:
n = A variable for the amount of data.

O(1) = Constant time, takes the same amount of time regardless of data.
O(log n) = Logarithmic time, becomes more efficient the larger the amount of data.
O(n) = Linear time, increases time proportionally to the amount of data
0(n log n) = Quasilinear time, it's similar to linear time for the most part, but gets less and less efficient the larger the dataset.
O(n^2) = Quadratic time, becomes much less efficient the larger the amount of data.
0(n!) = Factorial time, gets extremely less efficient the larger the amount of data.

Recursion = Applying the result of a procedure, to a procedure.
A recursive function calls itself, it can substitute iteration.
Commonly used in advanced sorting algorithms and navigating trees.

To write a recursive function, we must first define a base case, that is, a case where the function will stop calling itself.
Each recursive call uses the return value of the next deeper call to compute its own return value.
When we reach the base case, the return value of the base case is used by the previous function on the call stack
to determine its own return value, until the first function in the call stack returns its value, which is the final result.

If the base case takes too many recursive calls to be reached, and the call stack exceeds the maximum amount of functions it can hold
a Stack Overflow will happen which usually either triggers an exception, or the program just crashes/become unresponsive.

Advantages:
Easier to read/write
Easier to debug

Disadvantages:
Sometimes slower
uses more memory


----------------------DATA STRUCTURES-------------------------------------------------------------------------------------------------------


STACK
LIFO data structure (Last in first out).
Stores objects as a "vertical tower", like a stack of books or cds.
push() method is used to add objects to the top.
pop() method is used to remove from the top.

QUEUE
FIFO data structure (First in first out)
A collection designed for holding elements prior to processing.
Linear data structure (a data structure that stores data sequentially)
Head = First element of the queue
Tail = Last element of the queue
enqueue = adding an object to the end of the queue
dequeue = removing an object from the head of the queue

PRIORITY QUEUE
A FIFO data structure that serves elements with the highest priority first before elements with lower priority
Its like a normal queue, but you sort the elements first based on a parameter, so you can serve the elements
with the highest priority first and work your way down to the lowest priority.

LINKED LISTS
Stores nodes in 2 parts (data + address)
Nodes are in non-consecutive memory locations, elements are linked using pointers

Singly LinkedList structure (Address points only to the next node)
     Node                 Node                Node
[data | address] ->  [data | address] -> [data | address]

Doubly LinkedList structure (Address points to the previous and next node)
           Node                            Node
[address | data | address] <->  [address | data | address]

Advantages:
1 - Dynamic Data Structure (allocates needed memory while running)
2 - Insertion and Deletion of Nodes is easy, 0(1) constant time complexity
3 - No/Low Memory Waste

Disadvantages:
1 - Greater memory usage(additional pointer for the address, even more so with a doubly linked list)
2 - No random access of elements (no index[i])
3 - Accessing/searching elements is more time-consuming 0(n) linear time complexity

Use examples:
1 - Implement Stacks and Queues
2 - GPS navigation
(If you have a starting position and a final destination, every stop along the way can be seen as a node,
 and changing the path is as simple as changing, inserting or deleting a node)
3 - Music Playlists

DYNAMIC ARRAYS

Static Array = new String[capacity];
Can access elements directly with 0(1) constant time, but searching takes more time the larger the amount of elements, O(n) linear time complexity.
Has a fixed capacity of elements.

Dynamic arrays have their own static array with a fixed size, but once its inner static array reaches capacity, a dynamic array will instantiate
a new array with an increased capacity, copying the elements over to the new array

Advantages:
1 - Random memory access
2 - Good data cache utilization due to contiguous memory addresses
3 - Easy to insert/delete at tail due to no shifting of elements

Disadvantages:
1 - Memory wastage compared to LinkedLists due to increases in array capacity to accommodate more elements not being the exact number of new elements
leading to null memory addresses
2 - Shifting of elements has O(n) linear time complexity due to all elements needing to be shifted to the left in case of a deletion, or to the right
in case of an insertion.
3 - Expanding/Shrinking the array has 0 (n) linear time complexity due to needing to copy all the elements over to a new array with a new capacity

HASH TABLES
A data structure that stores unique keys to values
Each key/value pair is known as an Entry
Fast insertion, look up and deletion of key/value pairs
Not ideal for small data sets due to overhead, but great with large data sets.

Hashing = takes a key and computes an integer (formula will vary based on key and data type)
In a HashTable, we use the hash % (module) capacity to calculate an index number
key.hashCode() % capacity = index

Bucket = An indexed storage location for one or more Entries
can store multiple Entries in case of a collision(in that case, each bucket is treated like a LinkedList)

Collision = hash function results in the same index for more than one key, fewer collisions results in higher efficiency.

Runtime complexity = Best case 0(1) when no collisions, Worst case, where there's only collision, 0(n)

GRAPHS
Nonlinear aggregation of nodes and edges
Node(also known as a Vertex) = May contain some piece of data
Edge = A connection between two nodes

Undirected graphs:
A social network can be used as an example, each node can represent a user,
and if one user is friends with another user we can establish a connection an edge a connection between the two nodes.
If two nodes are connected they have ADJACENCY.

Directed graphs:
A directed graph contains one way edges that will connect one node to another.
A node can point to another node. that is a single edge.
A node can point to another node, and have the node it points to, also point to it, that is a double edge.
An example would be a street map, where each node is a possible destination, single edges could represent one way streets,
and double edges could represent two-way streets.

There are two ways to represent graphs, an Adjacency Matrix, and an Adjacency List.

Adjacency Matrix:
A 2D array with one row and one column for each node, if we need to check if there's adjacency, we would first find the
index of the node we are starting at, and then find the index of the node we are traveling to. (Ex: Row A (Starting node) Column B(Destination)).
If there is no adjacency(no edges) [A][B] would be 0, if there is adjacency, it would be 1.

Advantages:
The runtime complexity is constant 0(1), we simply have to find two indexes, the row and the column

Disadvantages:
O(n^2) quadratic space complexity, since each new node implies another row-column, 5 nodes results in 25 spaces in the matrix.

Adjacency matrices tent to suit graphs with a high amount of edges.

Adjacency List:
An adjacency list is an array of linked lists, each element being a separate linked list.
Each header within a linked list contains a node, if there is adjacency, we add the adjacent node to the linked list.

Advantages:
Uses less space than a matrix, 0(V+E) V being the number of vertices(nodes), and E for the number of edges

Disadvantages:
0(n) linear time complexity to find adjacency, since we must traverse linearly each linked list to check for adjacency.

Graphs are a popular way to model networks, which don't necessarily have any sort of order;

----------------------SORTING ALGORITHMS-------------------------------------------------------------------------------------------------------

BUBBLE SORT
Pairs of adjacent elements are compared, and the elements are swapped if they aren't in order
Its slow as hell, but very simple, so it's mostly used for educational purposes as an introduction to sorting algorithms.
In small data sets its passable but starts becoming extremely inefficient the larger the data set becomes.
Time Complexity = 0(n^2)

SELECTION SORT
Search through an array and keep track of the minimum value during each iteration, at the end of each iteration, we swap variables.
It's a bit more efficient than bubble sort, since it does fewer swaps per iteration.
Still, it becomes increasingly less efficient as the data set grows, used mostly for educational purposes.
Time complexity = 0(n^2)

INSERTION SORT
Starting from index 1, put the element on a temporary variable (temp)
compare the elements to the left and shift elements to the right if they are larger than what is in temp
Then put the temp element on the opening left after shifting the elements to the right.
Time complexity = O(n^2), though it can also run at 0(n) in the best case scenario where all the elements are already sorted.
Fewer steps than bubble sort, and runs in O(n) if the elements are already sorted.

MERGE SORT
A "divide and conquer" type of algorithm
Recursively divides an array by 2 (left and right arrays), until the size of the arrays is 1,
then compare the left and right arrays putting them in order on the array they came from (sorting, then merging)
Time Complexity = O(n log n)
Space Complexity = 0(n)

QUICK SORT
Sets a pivot, then moves the smaller elements to the left of the pivot.
Recursively divides an array in 2 partitions (one with the elements smaller than the pivot, and another with the elements bigger than the pivot).
Differently from merge sort, quick sort uses the initial pivot to determine the beginning and end of the partitions.
Time complexity = O(n log n) best and average cases, worst case, where the array is sorted or closed to be sorted, it runs in 0(n^2)
Space complexity = 0(log(n))


----------------------SEARCH ALGORITHMS-------------------------------------------------------------------------------------------------------

LINEAR SEARCH

Linear search = iterate through a collection one element at a time
Time complexity = O(n)

Advantages:
Fast for searches in small to medium data sets
Doesn't need to be sorted
Useful for data structures that don't have random access, like LinkedLists

Disadvantages:
Slow for large data sets

BINARY SEARCH

Binary search = finds the position of a target value within a sorted array, half of the array is eliminated during each "step".
Time complexity = 0(log n)

Advantages:
Gets more efficient the higher the amount of data

Disadvantages:
Dataset has to be sorted beforehand
Can be slower than linear search in small datasets

INTERPOLATION SEARCH

Interpolation search = Improvement over binary searches used for uniformly distributed data
It "guesses" where a value might be based on calculated probe results, if the probe is incorrect,
search area is narrowed and a new probe is calculated.

It basically guesses where the value is and returns the index

Time complexity:
Average case: O(log(log(n))
Worst case: O(n) values increase exponentially

DEPTH FIRST SEARCH:
A search algorithm for traversing a tree or graph data structure.
Pick a route, and keep going until you reach a dead end or an already visited node,
then backtrack to a previous node with unvisited adjacent neighbors
Traverse a graph branch ny branch
Utilizes a stack
Better if destination is on average far from the start
Children are visited before children


BREADTH FIRST SEARCH:
Also a search algorithm to traverse trees/graphs.
This is done one "Level" at a time instead of one "Branch" at a time, like in DFS.
Traverses a graph level by level.
Utilizes a Queue
Better if destination is on average close to start
Siblings are visited before children





